[
["index.html", "Statistics, Models and Machine Learning 1 Introduction 1.1 How to read this book 1.2 Notation and nomeclature used in this book 1.3 Supervised vs Unsupervised Models", " Statistics, Models and Machine Learning Michoel Snow 1 Introduction Should I be using linear regression, logistics regression, SVM, neural networks or one of at least a dozen other statistical/machine learning algorithms when trying to model my data? This is the fundamental question which has driven the authorship of this book. The problem is that the answer to that question for the most part is always the same, it depends. The answer depends not only on the data itself, but also why you are modeling in the first place, i.e., what question are you trying to answer with your model, as well as a whole host of other factors. The purpose of this book is to help you identify the important factors for choosing a model, given those factors which model(s) you should choose and why, and then finally how to implement that model in R or python. 1.1 How to read this book This book is broken up by model, ordered by computational complexity. Every chapter begins with a series of dataset visualizations and associated questions which are best answered through the models described in that chapter. Each chapter is intended to be modular and for the most part, you can read any of the chapters in any order. When multiple models utilize statistics or statistical techniques, the technique will be discussed the first time it is mentioned and all later mentions will refer the reader to where in the book they can find an explanation of that technique. 1.2 Notation and nomeclature used in this book We are all aware of the need for precision in mathematics and computation so as not to create unintentional errors, but I feel the same is true for language, especially in the description of mathematics and computation. For that reason I will strive to be consistent in my use of terminology, use the simplest possible accurate term and use as few terms as possible. For the sake of those who read other books on the subject, whenever a new term is introduced I will try to give all other commonly used names for that term. I will also bold a term whenever I give its definition. Note that when these rules disagree with standard agreed upon nomenclature, I will defer to the standard nomenclature, but will then explicitly define the terms to avoid confusion. Mathematical modeling lies at the heart of statistical learning, machine learning and any other name dreamt up by a marketing department. So instead of trying to define where one field ends and the other begins, or which technique is machine learning and which is statistics I am simply going to use the term model. In this book any relationship between variables is a model. A variable is a symbol which contains one or more known or unknown values. When the variable can contain any number of subset variables I will use the bold typeface and capital letters to represent it, such as \\(\\mathbf{X}\\) or \\(\\mathbf{Y}\\). Variables which contain no subset variables will be written using the regular typeface and capital letters, such as \\(X\\) or \\(Y\\). Individual values of a variable known as observations, also referred to as samples, will be written using a regular typeface and lowercase letters, such as \\(x\\) or \\(y\\). Indvidual observations for multiple variable will be written using the bold typeface and lowercase letters, such as \\(\\mathbf{X}\\) or \\(\\mathbf{y}\\). Numbered subscripts will refer to a specific subset or observation and lettered subscripts will be used for generic subsets or observations. When referring to both individual observations and subset variables, the first subscript is the observation and the second is the variable. The generic total number of observations is \\(n\\) and the generic total number of variables is \\(p\\). THis will make more sense when put together in an example. Let us say I am building a model which takes as its first four inputs the GPA, major, minor and high school of every student in Cornell’s graduating class of 1900. \\(\\mathbf{X}\\) refers to the set of all input variables. \\(X_1\\) refers to the GPA of all students and \\(X_2\\) refers to their major. \\(\\mathbf{x}_1\\) refers to all the inputs for the first student and \\(\\mathbf{x}_i\\) refers to the set of inputs for some generic \\(i\\)th student. \\(x_{3,1}\\) refers to the GPA for the third student. You can also think of this as a table where the rows are the observations and the columns are the subset variables. In matrix format this would look like: \\[\\mathbf{X} = \\left(X_1\\; X_2\\; X_3\\; \\ldots \\; X_p\\right)= \\left(\\begin{array} {c} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\mathbf{x}_3 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{array}\\right) = \\left(\\begin{array} {rrr} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; \\ldots &amp; x_{1,p}\\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; \\ldots &amp; x_{2,p}\\\\ x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; \\ldots &amp; x_{3,p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n,1} &amp; x_{n,2} &amp; x_{n,3} &amp; \\ldots &amp; x_{n,p}\\\\ \\end{array}\\right)\\] Unlike variables which are one or more unknowns, values are unknowns which always only take on a single value and will be represented with a lowercase letter, e.g., \\(a\\). Values which are associated with variables will share their subscripts, e.g., a value, \\(b\\), associated with variable \\(X_i\\), will be referenced as \\(b_i\\). A value associated with a specific variable is referred to as its coefficient. A function is a rule for taking an input and returning an output and is represented with a lower case letter followed by a pair of parentheses surrounding the input, e.g., \\(f(input)\\). Variables which are part of a function’s input are not suprisingly input variables, these are also referred to as indepedent variables, predictors or features. Similarly variables in the output will be termed output variables, but are also called dependent variables or responses. Using the model of a straight line, whose equation is \\[Y = mX + b\\] I can rewrite this as a function of the variable \\(X\\) as \\[ f(X) = mX + b\\] where \\(f(X) = Y\\). When building models often times we are only estimating the value of a coefficient or variable and don’t know its actual value. To distinguish estimates of variables and coefficients from their true counterparts, we put tiny hats on them, e.g., \\(\\hat{\\mathbf{Y}}\\), \\(\\hat{x}_{i,j}\\) or \\(\\hat{a}_0\\). 1.3 Supervised vs Unsupervised Models There are many ways to subdivide the various mathematical models, but for the topics covered within this book the dichotomy is made between supervised and unsupervised models. In Supervised models the data usually consists of observations where for every observation there is a paired input and output. In supervised modeling you are often given a training dataset which consists of entries for both the inputs and the outputs, as well as a test dataset, which only contains inputs from which you must predict outputs. The goal in supervised modeling is to create a function which accurately transforms the inputs into the outputs. In general the output only consists of a single variable but the input can consist of one or more variables. The standard nomenclature is to use one symbol for the output and one symbol for the input. In unsupervised learning the data is unlabeled, i.e., it is not split into inputs and outputs and the goal is to learn some desired feature about the data. A classic problem in unsupervised learning, is clustering, wherein the goal is to split the data into groups such that data within each group is more similar to each other than to data in other groups. 1.3.1 Solving Supervised Models In general, the goal of supervised models is to design a function which can predict a desired output variable \\(\\mathbf{Y}\\), given a set of input variables \\(\\mathbf{X}\\). To this end you are often given two datasets. The first dataset consists of inputs and their associated outputs, referred to as the training data. This training data is used to build the supervised model. This model is then used to make predictions given the second dataset, which is usually composed solely of never before seen inputs, called the test data. There is an assumption that there is some relationship between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) such that the output variable \\(\\mathbf{Y}\\) is a function of some fixed, but unknown function of \\(\\mathbf{X}\\), written as \\(f(\\mathbf{X})\\): \\[\\begin{equation} \\mathbf{Y} = f(\\mathbf{X}) + \\epsilon \\tag{1.1} \\end{equation}\\] \\(\\epsilon\\) represents the inherent independent random error present in the world, which has a mean of zero. However, the problem is that while we assume this relationship exists, we do not know what the function \\(f(\\mathbf{X})\\) is. To that end, supervised models seek to estimate this function using the training data. Using the estimate of the function we are able to make predictions of the output values given the input values in the test data. \\[\\begin{equation} \\hat{\\mathbf{Y}} = \\hat{f}(\\mathbf{X}) \\tag{1.2} \\end{equation}\\] Note that both the function and the output variable are indicated as estimates (through their tiny hats), but the input variable is not indicated as an estimate. This is because we know what the true inputs are we just don’t know what the function that relates the inputs to the outputs truly is and following from that we do not know what the output truly is for any given input, hence the tiny hats. ** START FROM HERE** However, note that unlike equation (1.1) When estimating the function \\(f()\\), there are two types of errors to consider % ggplot(aes(Age,Height)) + geom_jitter() +geom_line(data=NH_2,aes(Age,Height)) --> The implied assumption of this formulation is that the output can be perfectly predicted by the uknown function as applied to the inputs. However, due to a variety of factors there will almost always be some degree of error in the modelmost data gathered in the real world can not be perfectly fit to any model there is almost always some degree of error in most models and almost no function is perfectly accurate. So for almost any dataset the best fomulation is that it assumes a perfect model with no accounting for variation or error, and almost any model . To understand why this is a problem here is % mutate(Inc = 10000 + (Income-1)*5000) --> % ggplot(aes(Edu,Income)) + geom_point() --> "],
["lin-reg.html", "2 Linear Regression 2.1 Overview 2.2 Coefficient Calculation 2.3 Variable parameters", " 2 Linear Regression 2.1 Overview Almst all supervised models relate one or more input variables, \\(X_1,X_2, \\ldots , X_p\\), to an output variable, \\(Y\\). The general equation for a supervised model takes the form \\[Y = f(X)\\]. The simplest method with which to combine the inputs \\(X\\) is through a linear combination, namely addition. When dealing with a numerical output, we’ll deal with a different type of output in chapter NEED CHAPTER NAME, this problem is termed regression, hence the name linear regression. Before tackling the general form of the model let’s take a look at the example where there is only a single input variable, termed simple linear regression. Plotted in the figure below are 15 random temperature values read from an accurate celsius thermometer on the X axis and from a slightly less accurate farenheit thermometer on the Y axis. There is an obvious a linear relationship between the values on the X and Y axes, i.e., as the temperature in Celsius increases the temperature in Farenheit increases a proportional amount, and if you remember the formula to convert between the two is \\(F = \\dfrac{9}{5}\\cdot C + 32\\). Converting this formulation into our nomenclature gives the formula for simple linear regression \\[\\begin{equation} Y = a_0 + a_1 \\cdot X_1 \\tag{2.1} \\end{equation}\\] Where \\(Y\\) is the output variable, \\(X\\) is input variable and \\(a_0\\) and \\(a_1\\) are the coefficients. However, this equation is not actually correct, as the Farneheit value are not precisely equation to \\(\\dfrac{9}{5}\\) times the Celsius vcalues plus 32. This is very easy to see if I overlay a line generated from the equation \\(F = \\dfrac{9}{5}\\cdot C + 32\\) on top of the previous plot. The red bars from the points to the line represent the distance from the real values to the measured values, also know as the error. So I can rewrite Equation (2.1) in one of two ways to be accurate. The first is to include an error term, \\(\\epsilon\\) by standard notation, whose values are the lengths of the individual red lines. \\[\\begin{equation} Y = a_0 + a_1 \\cdot X_1 + \\epsilon \\tag{2.2} \\end{equation}\\] \\(\\epsilon\\) is a variable with the same number of values as \\(X\\) and \\(Y\\), as seen in the table below Celsius Farenheit (actual) Farenheit (recorded) Error -47 -52.6 -41.7 10.9 -38 -36.4 -51.8 -15.4 -35 -31.0 -36.2 -5.2 -31 -23.8 -25.8 -2.0 -19 -2.2 21.7 23.9 3 37.4 39.2 1.8 The other way or rewriting Equation (2.1) is by admitting that the output variable, as well as the coefficients are just estimates of their true values, which I can represent by giving them tiny hats. \\[\\begin{equation} \\hat{Y} = \\hat{a}_0 + \\hat{a}_1 \\cdot X_1 \\tag{2.3} \\end{equation}\\] When more input variables are added the simple part of the term is dropped and it is referred to as general regression and takes the form \\[\\begin{equation} Y = a_0 + a_1\\cdot X_1 + a_2\\cdot X_2 + \\ldots a_p\\cdot X_p + \\epsilon = a_0 + \\sum\\limits_{i=1}^p(a_i\\cdot X_i) + \\epsilon \\tag{2.4} \\end{equation}\\] Just as in simple linear regression, the \\(a\\)’s in front of the input variables control the slope of the line, \\(a_0\\) controls the intercept of the line. Taken together the \\(a\\)’s are referred to as the coefficients. \\(\\epsilon\\) is the error as measured by the distance from the points on the right hand side to the true values on the left hand side of the equation. Going back to our original simple linear regression problem involving temperature measurments. Since we know that the relationship between the input and output variables is not strictly goverened by the equation \\(F = \\dfrac{9}{5}\\cdot C + 32\\), we need a method of determining the coefficients which actually govern the relationship between our given input and outputs. 2.2 Coefficient Calculation 2.2.1 Least Sqaures The general idea of the least squares method is that you want to pick coefficents which minimize the differences between the given output and the output calculated from the right hand side of equation (2.1). This difference between the calculated output variable, \\(\\hat{Y}\\), and the actual output variable, \\(Y\\), is called the residual which is represented by the symbol \\(e\\). \\[\\begin{equation} e = Y-\\hat{Y} = Y - \\left(\\hat{a}_0 + \\hat{a}_1\\cdot X_1\\right) \\tag{2.5} \\end{equation}\\] Each observation has its own residual, wherein the residual \\(e_i\\) represents the \\(i\\)th residual and refers to the difference between the \\(i\\)th actual output, \\(Y_i\\), and the \\(i\\)th predicted output, \\(\\hat{Y}_i = \\hat{a}_{0,i} + \\hat{a}_{1,i}\\cdot X_i\\). Remember that the \\(i\\)th input variable \\(X_i\\) does not get a hat, becuase it is the actual input variable and not an estimate of the input variable. So for a series of observations the total residual is \\[\\begin{equation} e = Y-\\hat{Y} = Y - \\left(\\hat{a}_0 + \\hat{a}_1\\cdot X_1\\right) \\tag{2.5} \\end{equation}\\] What makes this process least squares is that you calculate the difference using the sum of the squared differences, termed the residual sum of sqaures (RSS). Formally the residual sum of squares for the general form of linear regression in equation (2.4) can be written as : \\[\\begin{equation} RSS(a) = \\sum\\limits_{i=1}^N\\left(Y_i - \\left[a_0 + \\sum\\limits_{j=1}^p a_j\\cdot X_{i,j} \\right]\\right) \\tag{2.6} \\end{equation}\\] 2.3 Variable parameters The values of the input and output parameters in a linear regression model are as follows. - Input Values can be any of the following - numerical values, also referred to as quantitative values - Dummy va the difference between So to be more precise, I can rewrite to include the fact that the output values, which in this case is the temperature in Farenheit, is actually an estimate of I can rewrite the general formula for linear regression and this time include a term, \\(\\epsilon\\) by standard notation, to represent the error. which is almost get the general formula for simple linear regression. I am writing the relationship between the input and output variables as approxiamtely, because the readings were not completely accurate. as applied to our data is not completely which is almost get the general formula for simple linear regression. I am writing the relationship between the input and output variables as approxiamtely, because the readings were not completely accurate. Unlike our previous equation this time our equation (2.1) has an equals sign is the general formula for simple linear regression. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package [@R-bookdown] in this sample book, which was built on top of R Markdown and knitr [@xie2015]. "],
["literature.html", "3 Literature", " 3 Literature Here is a review of existing methods. "]
]
